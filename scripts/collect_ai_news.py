#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI News Collector - ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ¯æ—¥ã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ãƒ»è¦ç´„ã—ã€ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹
"""

import os
import json
import asyncio
import requests
import feedparser
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional

import anthropic
from utils import (
    setup_logging, 
    load_config, 
    save_json, 
    load_json,
    APIUsageTracker
)

class AINewsCollector:
    def __init__(self):
        self.api_key = os.getenv('ANTHROPIC_API_KEY')
        if not self.api_key:
            raise ValueError("ANTHROPIC_API_KEY environment variable is required")
            
        self.client = anthropic.Anthropic(api_key=self.api_key)
        self.usage_tracker = APIUsageTracker()
        self.logger = setup_logging()
        
        # è¨­å®šèª­ã¿è¾¼ã¿
        self.config = load_config()
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.reports_dir = Path('reports')
        self.docs_dir = Path('docs/reports')
        self.data_dir = Path('docs/data')
        
        for dir_path in [self.reports_dir, self.docs_dir, self.data_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
    
    def fetch_ai_news_feeds(self) -> List[Dict]:
        """AIé–¢é€£RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"""
        feeds = [
            'https://feeds.feedburner.com/oreilly/radar/ai',
            'https://techcrunch.com/category/artificial-intelligence/feed/',
            'https://www.artificialintelligence-news.com/feed/',
            'https://venturebeat.com/ai/feed/',
            'https://www.theverge.com/rss/ai-artificial-intelligence/index.xml'
        ]
        
        all_articles = []
        
        for feed_url in feeds:
            try:
                self.logger.info(f"Fetching feed: {feed_url}")
                feed = feedparser.parse(feed_url)
                
                for entry in feed.entries[:5]:  # å„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ€æ–°5ä»¶
                    # 24æ™‚é–“ä»¥å†…ã®è¨˜äº‹ã®ã¿
                    published = getattr(entry, 'published_parsed', None)
                    if published:
                        pub_date = datetime(*published[:6])
                        if datetime.now() - pub_date > timedelta(days=2):
                            continue
                    
                    article = {
                        'title': entry.get('title', ''),
                        'link': entry.get('link', ''),
                        'summary': entry.get('summary', '')[:300],
                        'published': entry.get('published', ''),
                        'source': feed.feed.get('title', 'Unknown'),
                        'tags': [tag.term for tag in getattr(entry, 'tags', [])]
                    }
                    all_articles.append(article)
                    
            except Exception as e:
                self.logger.error(f"Error fetching {feed_url}: {e}")
                continue
        
        # é‡è¤‡é™¤å»ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰
        seen_titles = set()
        unique_articles = []
        for article in all_articles:
            title_clean = article['title'].lower().strip()
            if title_clean not in seen_titles:
                seen_titles.add(title_clean)
                unique_articles.append(article)
        
        self.logger.info(f"Collected {len(unique_articles)} unique articles")
        return unique_articles[:15]  # æœ€å¤§15è¨˜äº‹
    
    async def create_summary_with_batch(self, articles: List[Dict]) -> Optional[str]:
        """Batch API + ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ãŸè¦ç´„ç”Ÿæˆ"""
        
        if not articles:
            return None
            
        articles_text = "\\n\\n".join([
            f"ã€{i+1}ã€‘ {article['title']}\\n"
            f"ã‚½ãƒ¼ã‚¹: {article['source']}\\n"
            f"è¦ç´„: {article['summary']}\\n"
            f"URL: {article['link']}"
            for i, article in enumerate(articles)
        ])
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾è±¡ã®ãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        cached_prompt = """
ä»¥ä¸‹ã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åˆ†æã—ã¦ã€æ—¥æœ¬èªã§åŒ…æ‹¬çš„ãªè¦ç´„ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

# å‡ºåŠ›å½¢å¼

## ğŸ”¥ ä»Šæ—¥ã®æ³¨ç›®AIãƒ‹ãƒ¥ãƒ¼ã‚¹

### â­ æœ€é‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹
æœ€ã‚‚é‡è¦ã§å½±éŸ¿åŠ›ã®å¤§ãã„ãƒ‹ãƒ¥ãƒ¼ã‚¹1-2ä»¶ã‚’è©³ã—ãè§£èª¬ã—ã¦ãã ã•ã„ã€‚

### ğŸ“ˆ æ³¨ç›®ãƒˆãƒ¬ãƒ³ãƒ‰  
æ¥­ç•Œã§è©±é¡Œã«ãªã£ã¦ã„ã‚‹é‡è¦ãªãƒ‹ãƒ¥ãƒ¼ã‚¹2-3ä»¶ã‚’ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

### ğŸ’¡ ãã®ä»–ã®è©±é¡Œ
ãã®ä»–ã®èˆˆå‘³æ·±ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚„ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ç´¹ä»‹ã—ã¦ãã ã•ã„ã€‚

## ğŸ“Š æŠ€è¡“ãƒ»å¸‚å ´åˆ†æ

### æŠ€è¡“çš„é€²æ­©
ä»Šæ—¥ç™ºè¡¨ã•ã‚ŒãŸæŠ€è¡“çš„ãªé€²æ­©ã‚„é©æ–°ã«ã¤ã„ã¦åˆ†æã—ã¦ãã ã•ã„ã€‚

### å¸‚å ´ãƒ»ãƒ“ã‚¸ãƒã‚¹ã¸ã®å½±éŸ¿
ä¼æ¥­æ´»å‹•ã€æŠ•è³‡ã€å¸‚å ´å‹•å‘ã¸ã®å½±éŸ¿ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚

## ğŸ”® ä»Šå¾Œã®å±•æœ›
ã“ã‚Œã‚‰ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰è¦‹ãˆã‚‹ä»Šå¾Œã®AIæ¥­ç•Œã®å‹•å‘ã‚„æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆã‚’äºˆæ¸¬ã—ã¦ãã ã•ã„ã€‚

---

# æŒ‡ç¤º
- å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å…·ä½“çš„ã§åˆ†ã‹ã‚Šã‚„ã™ãè¨˜è¿°
- é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã¯**å¤ªå­—**ã§å¼·èª¿  
- èª­ã¿æ‰‹ã®é–¢å¿ƒã‚’å¼•ãé­…åŠ›çš„ãªæ–‡ç« ã§
- æŠ€è¡“çš„ãªå†…å®¹ã‚‚ä¸€èˆ¬èª­è€…ã«åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜
"""
        
        try:
            # Batch API ãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆ
            batch_request = {
                "custom_id": f"ai-news-{datetime.now().strftime('%Y%m%d')}",
                "method": "POST",
                "url": "/v1/messages",
                "body": {
                    "model": "claude-3-5-sonnet-20241022",
                    "max_tokens": 3000,
                    "messages": [
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": cached_prompt,
                                    "cache_control": {"type": "ephemeral"}
                                },
                                {
                                    "type": "text",
                                    "text": f"\\n\\n=== ä»Šæ—¥åé›†ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ ===\\n{articles_text}"
                                }
                            ]
                        }
                    ]
                }
            }
            
            # ãƒãƒƒãƒã‚¸ãƒ§ãƒ–é€ä¿¡
            self.logger.info("Submitting batch job...")
            batch_job = self.client.batches.create(requests=[batch_request])
            
            # å‡¦ç†å®Œäº†ã¾ã§å¾…æ©Ÿ
            max_wait_time = 600  # 10åˆ†
            wait_time = 0
            
            while wait_time < max_wait_time:
                status = self.client.batches.retrieve(batch_job.id)
                self.logger.info(f"Batch status: {status.request_counts}")
                
                if status.request_counts.completed > 0:
                    # çµæœå–å¾—
                    result_response = self.client.batches.list_results(batch_job.id)
                    if result_response.data:
                        result = result_response.data[0]
                        if hasattr(result, 'result') and result.result:
                            summary = result.result.content[0].text
                            
                            # ä½¿ç”¨é‡è¨˜éŒ²
                            usage = getattr(result.result, 'usage', None)
                            if usage:
                                self.usage_tracker.log_usage(
                                    input_tokens=usage.input_tokens,
                                    output_tokens=usage.output_tokens,
                                    cache_hit=getattr(usage, 'cache_read_input_tokens', 0) > 0,
                                    batch_mode=True
                                )
                            
                            return summary
                elif status.request_counts.failed > 0:
                    self.logger.error("Batch job failed")
                    break
                    
                await asyncio.sleep(30)
                wait_time += 30
            
            self.logger.warning("Batch job timed out, falling back to regular API")
            return await self.create_summary_regular(articles)
            
        except Exception as e:
            self.logger.error(f"Batch API error: {e}")
            return await self.create_summary_regular(articles)
    
    async def create_summary_regular(self, articles: List[Dict]) -> Optional[str]:
        """é€šå¸¸APIã§ã®è¦ç´„ç”Ÿæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        
        articles_text = "\\n\\n".join([
            f"ã€{i+1}ã€‘ {article['title']}\\n"
            f"ã‚½ãƒ¼ã‚¹: {article['source']}\\n"
            f"è¦ç´„: {article['summary']}\\n"
            f"URL: {article['link']}"
            for i, article in enumerate(articles)
        ])
        
        prompt = f"""
ä»¥ä¸‹ã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åˆ†æã—ã¦ã€æ—¥æœ¬èªã§åŒ…æ‹¬çš„ãªè¦ç´„ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

{articles_text}

# å‡ºåŠ›å½¢å¼

## ğŸ”¥ ä»Šæ—¥ã®æ³¨ç›®AIãƒ‹ãƒ¥ãƒ¼ã‚¹

### â­ æœ€é‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹
### ğŸ“ˆ æ³¨ç›®ãƒˆãƒ¬ãƒ³ãƒ‰  
### ğŸ’¡ ãã®ä»–ã®è©±é¡Œ

## ğŸ“Š æŠ€è¡“ãƒ»å¸‚å ´åˆ†æ

### æŠ€è¡“çš„é€²æ­©
### å¸‚å ´ãƒ»ãƒ“ã‚¸ãƒã‚¹ã¸ã®å½±éŸ¿

## ğŸ”® ä»Šå¾Œã®å±•æœ›

é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã¯**å¤ªå­—**ã§å¼·èª¿ã—ã€èª­ã¿æ‰‹ã®é–¢å¿ƒã‚’å¼•ãé­…åŠ›çš„ãªæ–‡ç« ã§ãŠé¡˜ã„ã—ã¾ã™ã€‚
"""
        
        try:
            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=3000,
                messages=[{"role": "user", "content": prompt}]
            )
            
            summary = response.content[0].text
            
            # ä½¿ç”¨é‡è¨˜éŒ²
            self.usage_tracker.log_usage(
                input_tokens=response.usage.input_tokens,
                output_tokens=response.usage.output_tokens,
                cache_hit=False,
                batch_mode=False
            )
            
            return summary
            
        except Exception as e:
            self.logger.error(f"Regular API error: {e}")
            return None
    
    def save_report(self, summary: str, articles: List[Dict]) -> str:
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’Markdownãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜"""
        today = datetime.now().strftime('%Y-%m-%d')
        
        # ãƒ¡ã‚¤ãƒ³ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        report_content = f"""# ğŸ¤– AI News Daily Report - {today}

{summary}

---

## ğŸ“° åé›†ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§

"""
        
        for i, article in enumerate(articles, 1):
            report_content += f"""
### {i}. {article['title']}

- **ã‚½ãƒ¼ã‚¹**: {article['source']}
- **å…¬é–‹æ—¥**: {article.get('published', 'N/A')}
- **ãƒªãƒ³ã‚¯**: [è¨˜äº‹ã‚’èª­ã‚€]({article['link']})
- **æ¦‚è¦**: {article['summary']}

"""
        
        report_content += f"""
---

**ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ™‚åˆ»**: {datetime.now().strftime('%Y/%m/%d %H:%M:%S')}  
**å‡¦ç†è¨˜äº‹æ•°**: {len(articles)}ä»¶  
**ã‚·ã‚¹ãƒ†ãƒ **: GitHub Actions + Claude API
"""
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        report_path = self.reports_dir / f"ai-news-{today}.md"
        docs_report_path = self.docs_dir / f"ai-news-{today}.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        with open(docs_report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        self.logger.info(f"Report saved: {report_path}")
        return str(report_path)
    
    def send_notifications(self, summary: str, report_url: str):
        """é€šçŸ¥ã‚’é€ä¿¡"""
        
        # Slacké€šçŸ¥
        slack_webhook = os.getenv('SLACK_WEBHOOK_URL')
        if slack_webhook:
            try:
                short_summary = summary[:800] + "..." if len(summary) > 800 else summary
                payload = {
                    "text": f"ğŸ“° *AI News Daily - {datetime.now().strftime('%Y/%m/%d')}*\\n\\n{short_summary}\\n\\nğŸ“Š è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_url}",
                    "username": "AI News Bot",
                    "icon_emoji": ":robot_face:"
                }
                
                response = requests.post(slack_webhook, json=payload, timeout=10)
                if response.status_code == 200:
                    self.logger.info("Slack notification sent successfully")
                else:
                    self.logger.warning(f"Slack notification failed: {response.status_code}")
            except Exception as e:
                self.logger.error(f"Error sending Slack notification: {e}")
        
        # Discordé€šçŸ¥
        discord_webhook = os.getenv('DISCORD_WEBHOOK_URL')
        if discord_webhook:
            try:
                embed = {
                    "embeds": [{
                        "title": f"ğŸ¤– AI News Daily - {datetime.now().strftime('%Y/%m/%d')}",
                        "description": summary[:2000],
                        "color": 3447003,
                        "timestamp": datetime.now().isoformat(),
                        "footer": {"text": "Powered by GitHub Actions + Claude API"},
                        "fields": [
                            {"name": "ğŸ“Š è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ", "value": f"[Webã‚µã‚¤ãƒˆã§èª­ã‚€]({report_url})", "inline": False}
                        ]
                    }]
                }
                
                response = requests.post(discord_webhook, json=embed, timeout=10)
                if response.status_code in [200, 204]:
                    self.logger.info("Discord notification sent successfully")
                else:
                    self.logger.warning(f"Discord notification failed: {response.status_code}")
            except Exception as e:
                self.logger.error(f"Error sending Discord notification: {e}")
    
    async def run_daily_collection(self):
        """æ¯æ—¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚’å®Ÿè¡Œ"""
        self.logger.info(f"Starting AI news collection: {datetime.now()}")
        
        try:
            # 1. ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†
            articles = self.fetch_ai_news_feeds()
            
            if not articles:
                self.logger.warning("No articles found, exiting")
                return
            
            # 2. è¦ç´„ç”Ÿæˆï¼ˆBatch APIå„ªå…ˆï¼‰
            summary = await self.create_summary_with_batch(articles)
            
            if not summary:
                self.logger.error("Failed to generate summary")
                return
            
            # 3. ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            report_path = self.save_report(summary, articles)
            
            # 4. çµ±è¨ˆæƒ…å ±ä¿å­˜
            stats = {
                "date": datetime.now().isoformat(),
                "articles_count": len(articles),
                "sources": list(set(article['source'] for article in articles)),
                "report_path": report_path,
                "usage": self.usage_tracker.get_daily_stats()
            }
            save_json(stats, f"reports/stats-{datetime.now().strftime('%Y-%m-%d')}.json")
            
            # 5. é€šçŸ¥é€ä¿¡
            report_url = f"https://hummer12345.github.io/ai-news-daily-collector/reports/ai-news-{datetime.now().strftime('%Y-%m-%d')}.html"
            self.send_notifications(summary, report_url)
            
            # 6. JSONãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆwebsiteç”Ÿæˆç”¨ï¼‰
            data = {
                "summary": summary,
                "articles": articles,
                "generated_at": datetime.now().isoformat(),
                "stats": stats
            }
            save_json(data, f"docs/data/latest.json")
            
            self.logger.info("Daily AI news collection completed successfully")
            
        except Exception as e:
            self.logger.error(f"Error in daily collection: {e}", exc_info=True)
            raise

if __name__ == "__main__":
    collector = AINewsCollector()
    asyncio.run(collector.run_daily_collection())